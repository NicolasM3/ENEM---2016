# -*- coding: utf-8 -*-
"""Notas de matemática ENEM-2016

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4R9RXOzdh4NKSJRXxqM7FhbOWiCO82L

# **Desafio codenation**

---

O contexto do desafio gira em torno dos resultados do ENEM 2016 (disponíveis no arquivo train.csv). Este arquivo, e apenas ele, deve ser utilizado para todos os desafios. Qualquer dúvida a respeito das colunas, consulte o [Dicionário dos Microdados do Enem 2016.](http://portal.inep.gov.br/artigo/-/asset_publisher/B4AQV9zFY7Bv/content/inep-divulga-os-microdados-do-enem-2016/21206)

Muitas universidades brasileiras utilizam o ENEM para selecionar seus futuros alunos e alunas. Isto é feito com uma média ponderada das notas das provas de matemática, ciências da natureza, linguagens e códigos, ciências humanas e redação, com os pesos abaixo:

matemática: 3
ciências da natureza: 2
linguagens e códigos: 1.5
ciências humanas: 1
redação: 3
No arquivo test.csv crie um modelo para prever nota da prova de matemática (coluna NU_NOTA_MT) de quem participou do ENEM 2016.

Salve sua resposta em um arquivo chamado answer.csv com duas colunas: NU_INSCRICAO e NU_NOTA_MT.

# **Bibliotecas**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor

plt.style.use('grayscale')

"""# **Verificando correlações**


---



O arquivo de valores train.csv contém uma quantidades enorme de dados, que podem ser ou não úteis para prever o nosso objetivo, as notas de matemática. Por isso antes de iniciar o nosso algoritmo, precisamos descobrir quais dados presentes nos arquivos podem ser usados em nossa previsão, separando os que mais parecem ter influência na nota final de matemática.
"""

df_train = pd.read_csv('train.csv', sep="," , encoding="UTF8" )
df_test = pd.read_csv('test.csv', sep="," , encoding="UTF8" )
df_resposta = pd.DataFrame()
print(set(df_test.columns).issubset(set(df_train.columns)))

"""Na 4 linha desse código verificamos se todas as colunas do arquivo train estão contidos no arquivo test, e vemos que sim. Agora vamos ver as correlações."""

df_train.corr()

"""OK! Isso retornou muitas linhas com muitos número. Vamos tentar colocar isso em um gráfico. Porém com essa quantidade de dados o gráfico irá ficar enorme, por isso procurei os dados que podem conter dados mais significantes para a previsão através do [Dicionário dos Microdados do Enem 2016.](http://portal.inep.gov.br/artigo/-/asset_publisher/B4AQV9zFY7Bv/content/inep-divulga-os-microdados-do-enem-2016/21206)

## Dados
"""

features_train = [
    'NU_NOTA_MT',
    'NU_NOTA_CN',
    'NU_NOTA_CH',
    'NU_NOTA_LC',
    'NU_NOTA_REDACAO',
    'NU_NOTA_COMP1',
    'NU_NOTA_COMP2',
    'NU_NOTA_COMP3',
    'NU_NOTA_COMP4',
    'NU_NOTA_COMP5']

features_test = [
    'NU_NOTA_CN',
    'NU_NOTA_CH',
    'NU_NOTA_LC',
    'NU_NOTA_REDACAO',
    'NU_NOTA_COMP1',
    'NU_NOTA_COMP2',
    'NU_NOTA_COMP3',
    'NU_NOTA_COMP4',
    'NU_NOTA_COMP5']

"""## Gráficos"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

corr = df_train[features_train].corr()
plt.subplots(figsize=(11, 8))
sns.heatmap(corr, annot=True, annot_kws={"size": 15}, linecolor='black', cmap='Greens')

"""Essa é a correlação entre os dados escolhidos.

# **Testando os dados**


---

Agora que já vimos os melhores dados para usar precisamos verificar se não há nenhum preoblema que possa interferir em nossos predição. A primeira coisa que é perceptível dando uma breve olhada nos dados, é que, tanto no train tanto no test, existem alguns dados nulos, que vão interferir em nossa análise.
"""

df_test[features_test].isnull().sum()

"""Para lidar com esses valores null é comum fazer uma das 3 coisas abaixo:

*   Aplicar a média nos dados
*   Zerar os dados nulos
*   Remover os dados nulos do dataset

Cada opção vai gerar resultados diferentes. Nesse teste vamos tentar remover as notas nulas do dataset
"""

df_test = df_test.loc[
      (df_test['NU_NOTA_CN'].notnull()) & 
      (df_test['NU_NOTA_CH'].notnull()) & 
      (df_test['NU_NOTA_LC'].notnull()) & 
      (df_test['NU_NOTA_REDACAO'].notnull())    
]

df_train = df_train.loc[
      (df_train['NU_NOTA_CN'].notnull()) & 
      (df_train['NU_NOTA_CH'].notnull()) & 
      (df_train['NU_NOTA_LC'].notnull()) & 
      (df_train['NU_NOTA_MT'].notnull()) &
      (df_train['NU_NOTA_REDACAO'].notnull())
]

df_test[features_test].isnull().sum()

"""Agora eliminamos todos os nulos.

# **Começando com os sklearn**

Agora que já padronizamos nossos dados podemos começar a usar a biblioteca sklearn para fazer o algoritmo para pre-dizer as nostas do conjunto de dadods test. Para começar vamos separar o target e fazer novas variáveis para utilizar no treinamento
"""

y_train = df_train['NU_NOTA_MT']
x_train = df_train[features_test]
x_test = df_test[features_test]

sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""Na linha 5 usamos um metodo de distribuição que aproxima um valor de 0 com uma variação de 1. Enquanto na linha 6 e 7 temos dois métodos diferente mais muito parecidos do StandardScaler para transformar os dados. Veja a diferença abaixo: 

fit(): usado para gerar parâmetros de modelo de aprendizado a partir de dados de treinamento

transform(): parâmetros gerados a partir do método fit(), aplicados no modelo para gerar o conjunto de dados transformados.

fit_transform(): combinação de fit() e transform() api no mesmo conjunto de dados

[Link dessa resposta no StackOverflow](https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn)

## Passando o RandomForestRegressor

O modelo que usaremos é randomForestRegressor
"""

regressor = RandomForestRegressor( 
           criterion='mae', 
           max_depth=8,
           max_leaf_nodes=None,
           min_impurity_split=None,
           min_samples_leaf=1,
           min_samples_split=2,
           min_weight_fraction_leaf=0.0,
           n_estimators= 500,
           n_jobs=-1,
           random_state=0,
           verbose=0,
           warm_start=False
)

"""Realizando nosso treino através do fit"""

regressor.fit(x_train, y_train)

"""Obtendo os resultados"""

y_pred_test = regressor.predict(x_test)

df_resposta['NU_INSCRICAO'] = df_test['NU_INSCRICAO']
df_resposta['NU_NOTA_MT'] = np.around(y_pred_test,2)

df_resposta.to_csv('answer.csv', index=False, header=True)
print('Finalizado')

"""# **BIBLIOGRAFIA:**

https://scikit-learn.org/stable/user_guide.html
https://minerandodados.com.br/pre-processamento-standartization/
https://medium.com/@wesleywatanabe/data-science-machine-learning-enem-regressao-linear-5cd140459dc3
https://medium.com/ensina-ai/machine-learning-randomforest-para-prever-nota-de-matem%C3%A1tica-do-enem-2016-8893b73882f4
"""